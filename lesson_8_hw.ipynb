{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Алгоритмы анализа данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Урок 8. Снижение размерности данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Практическое задание </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 1:__ Можно ли отобрать наиболее значимые признаки с помощью PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Опишем суть метода PCA в терминах преобразования линейного n-мерного пространства признаков, где n-количество признаков содержащихся в матрице признаков X нашей задачи.\n",
    "\n",
    "Пусть имеется некоторе n-мерное пространство $S$, в котором определён n-мерный базис, состоящих из n базисных векторов $e_i, i=1,...,n$.\n",
    "\n",
    "Для простоты и наглядности будем считать, что базис является ортонормированным, то есть $$e_i\\cdot e_j=\\delta_{ij}$$\n",
    "Здесь $\\delta_{ij}$ - дельта символ Кронекера\n",
    "$$\\delta_{ij}= \\left\\{_{0, i\\neq j}^{1, i=j}\\right.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть вектор $X_k$ - n-мерный вектор нашего пространства признаков, проекции которого $x_i^k$ на базис $e_i$ и есть __строка нашей матрицы признаков $X$__, то есть набор значений признаков соответствующих $y_k$, конкретному k-му значению вектора значений $y$. В нашем базисе вектор $X_k$ можно записать как $$X_k=\\sum_{i=1}^{n}x_i^k e_i$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зафиксируем, что нашей матрице параметров $X$, в нашем n-мерном пространстве, соответствует некоторая n-мерная гиперплоскость $G$. Точками этой гиперплоскости являются точки заданные векторами $X_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вращая наш базис $e_i$ в пространстве $S$, мы можем перейти в новый ортонормированнй базис $e'_i$. Каждый базисный вектор новой системы координат, может быть выражен как линейная комбинация базисных векторов старой системы координат\n",
    "$$e'_i=\\sum_{j=1}^{n}a_{ij} e_j$$\n",
    "и наоборот, каждый базисный вектор старой системы координат, может быть выражен как линейная комбинация базисных векторов новой системы координат\n",
    "$$e_i=\\sum_{j=1}^{n}{a'}_{ij} e'_j$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Матрицы перехода между базисами А и А' связаны между собой ($A'=A^T$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждый n-мерный вектор нашего пространства $X_k$, в новом базисе вектор $X_k$ можно записать как $$X_k=\\sum_{i=1}^{n} {x'}_i^k {e'}_i$$\n",
    "где ${x'}_i^k$ - координаты вектора $X_k$ в новой системе координат."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно выбрать такой базис, что часть базисных векторов новой системы координат окажутся \"ортогональными\" нашей n-мерной гиперплоскости параметров $G$. Пусть количество таких веторов равно $m$. \n",
    "\n",
    "Под \"ортогональнальностью\" мы будем понимать, то что проекция вектора параметров $X_k$ будет равна 0 или близка к нему, то есть ${x'}_s^k\\approx 0$ для некоторых m номеров из набора n.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соответственно, в этом случае, в новом базисе $e'$ мы можем перейти от n-мерного пространства, к подпространству размерности $n-m < n$, так как \n",
    "$$X_k=\\sum_{i=1}^{n-m} {x'}_i^k {e'}_i + \\sum_{i=m}^{n} {x'}_i^k {e'}_i=\\sum_{i=1}^{n-m} {x'}_i^k {e'}_i$$\n",
    "где \n",
    "$$\\sum_{i=m}^{n} {x'}_i^k {e'}_i=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть сделать именно то, что нам необходимо, понизить размерность до n-m не теряя качества модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом как я понимаю и заключается смысл метода главных компонент PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Вывод__\n",
    "\n",
    "Как показали наши рассуждения, метод PCA не выявляет наиболее значимые признаки, а позволяет понизить размерность переходом к новому базису и затем переходу к подпрастранству меньшей размерности, без потери качества модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
